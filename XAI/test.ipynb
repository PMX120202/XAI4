{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torch import nn, Tensor, optim\n",
    "import torch.nn.functional as F\n",
    "import util\n",
    "\n",
    "from pertubate import FadeMovingAverage\n",
    "from NbeatsODE.model import BeatsODE\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "parser.add_argument('--device', type=str, default='cuda', help='device to run the model on')\n",
    "parser.add_argument('--data', type=str, default='store/PEMS-BAY', help='data path')\n",
    "parser.add_argument('--adj_data', type=str, default='store/adj_mx_bay.pkl', help='adj data path')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=15, help='')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='batch size')\n",
    "\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0001, help='weight decay rate')\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.3, help='dropout rate')\n",
    "parser.add_argument('--print_every', type=int, default=50, help='')\n",
    "parser.add_argument('--save', type=str, default='saved_models/BeatsODE', help='save path')\n",
    "parser.add_argument('--log', type=str, default='log/dynamask', help='save path')\n",
    "\n",
    "parser.add_argument('--blackbox_file', type=str, default='save_blackbox/G_T_model_15.pth', help='blackbox .pth file')\n",
    "\n",
    "parser.add_argument('--num_nodes', type=int, default=207, help='number of nodes')\n",
    "parser.add_argument('--timestep', type=str, default=12, help='time step')\n",
    "parser.add_argument('--input_dim', type=str, default=2, help='channels')\n",
    "parser.add_argument('--output_dim', type=str, default=1, help='channels')\n",
    "parser.add_argument('--samples', type=str, default=10, help='samples data')\n",
    "parser.add_argument('--sample_iters', type=str, default=1000, help='___')\n",
    "parser.add_argument('--initial_mask_coeff', type=str, default=0.5, help='___')\n",
    "parser.add_argument('--reg_coeff', type=str, default=0.07, help='___')\n",
    "\n",
    "args = parser.parse_args('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "torch.Size([1, 12, 207, 2])\n",
      "MAE: 0.5915, MAPE: 0.6341, RMSE: 0.7255 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check(sample, arr):\n",
    "    for i, index in enumerate(arr): \n",
    "        sample[:, i, index, :]= 0.0\n",
    "\n",
    "sample = np.load('./saved_models/dynamask/sample_METR_LA_0.npz')['arr_0']\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(args.device)\n",
    "_, _, adj_mx = util.load_adj('store/adj_mx.pkl')\n",
    "\n",
    "adj_mx = torch.tensor(adj_mx).to(device)\n",
    "\n",
    "\n",
    "model = BeatsODE(in_dim=2, out_dim=1, seq_len=12).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('save_blackbox/G_T_model_15.pth'))\n",
    "\n",
    "\n",
    "print(\"start training...\", flush=True)\n",
    "his_loss = []\n",
    "val_time = []\n",
    "train_time = []\n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "mask = np.load('./saved_models/dynamask/saliency_METR_LA_0.npz')['arr_0']\n",
    "# for i, sample in enumerate(samples):\n",
    "sample= torch.Tensor(sample).unsqueeze(0)\n",
    "trainx = torch.Tensor(sample).to(device)\n",
    "print(trainx.shape)\n",
    "\n",
    "output = model(trainx.transpose(1, 3), adj_mx)\n",
    "output = output[:, :, :, 0:1]\n",
    "output= output.transpose(-3, -1)\n",
    "mask= mask[0, 0, :, :]\n",
    "mask= mask.T\n",
    "mask = torch.Tensor(mask)\n",
    "top_k = torch.topk(mask, 15, dim=1, sorted= True)\n",
    "index_ = top_k.indices\n",
    "arr= []\n",
    "for i in range(index_.shape[0]):\n",
    "        row= np.full((207, ), True)\n",
    "        row[index_[i]]= False\n",
    "        arr.append(row)\n",
    "arr = np.array(arr)\n",
    "\n",
    "check(sample, arr)\n",
    "trainx = torch.Tensor(sample).to(device)\n",
    "output2 = model(trainx.transpose(1, 3), adj_mx)\n",
    "output2 = output2[:, :, :, 0:1]\n",
    "output2= output2.transpose(-3, -1)\n",
    "MAE= util.masked_mae(output2, output, 0.0)\n",
    "MAPE= util.masked_mape(output2, output, 0.0)\n",
    "RMSE= util.masked_rmse(output2, output, 0.0)\n",
    "\n",
    "\n",
    "print(f'MAE: {MAE:.4f}, MAPE: {MAPE:.4f}, RMSE: {RMSE:.4f} \\n')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load data  store/adj_mx.pkl : [Errno 2] No such file or directory: 'store/adj_mx.pkl'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'store/adj_mx.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_models/METR_LA/sample_METR_LA_1.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 20\u001b[0m _, _, adj_mx \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstore/adj_mx.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m adj_mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(adj_mx)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m BeatsODE(in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\graduation_project\\XAI4\\XAI\\util.py:83\u001b[0m, in \u001b[0;36mload_adj\u001b[1;34m(pkl_filename)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_adj\u001b[39m(pkl_filename):\n\u001b[1;32m---> 83\u001b[0m     sensor_ids, sensor_id_to_ind, adj_mx \u001b[38;5;241m=\u001b[39m \u001b[43mload_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkl_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sensor_ids, sensor_id_to_ind, adj_mx\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\graduation_project\\XAI4\\XAI\\util.py:72\u001b[0m, in \u001b[0;36mload_pickle\u001b[1;34m(pickle_file)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pickle\u001b[39m(pickle_file):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickle_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     73\u001b[0m             pickle_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'store/adj_mx.pkl'"
     ]
    }
   ],
   "source": [
    "def check(sample, arr):\n",
    "    for i, index in enumerate(arr): \n",
    "        sample[:, i, index, :]= 0.0\n",
    "\n",
    "\n",
    "def normalize_tensor(data):\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    # X_scaled = normalized_data * (max_val - min_val) + min_val\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "sample = np.load('./saved_models/METR_LA/sample_METR_LA_1.npz')['arr_0']\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(args.device)\n",
    "_, _, adj_mx = util.load_adj('store/adj_mx.pkl')\n",
    "\n",
    "adj_mx = torch.tensor(adj_mx).to(device)\n",
    "\n",
    "\n",
    "model = BeatsODE(in_dim=2, out_dim=1, seq_len=12).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('save_blackbox/G_T_model_15.pth'))\n",
    "\n",
    "\n",
    "print(\"start training...\", flush=True)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "mask = np.load('./saved_models/METR_LA/saliency_METR_LA_1.npz')['arr_0']\n",
    "# for i, sample in enumerate(samples):\n",
    "sample= torch.Tensor(sample).unsqueeze(0)\n",
    "trainx = torch.Tensor(sample).to(device)\n",
    "print(trainx.shape)\n",
    "\n",
    "mask= mask[0, 0, :, :]\n",
    "# print(mask.shape)\n",
    "mask= normalize_tensor(mask)\n",
    "mask= mask.T\n",
    "mask = torch.Tensor(mask)\n",
    "# print(\"mask: \", mask)\n",
    "top_k = torch.topk(mask, 30, dim=1, sorted= True)\n",
    "index_ = top_k.indices\n",
    "arr= []\n",
    "for i in range(index_.shape[0]):\n",
    "        row= np.full((207, ), True)\n",
    "        row[index_[i]]= False\n",
    "        arr.append(row)\n",
    "arr = np.array(arr)\n",
    "\n",
    "amae= [0]*12\n",
    "amape= [0]*12\n",
    "armse= [0]*12\n",
    "temp = 10\n",
    "for iter in range(temp):\n",
    "        for hori in range(12):\n",
    "                with torch.no_grad():\n",
    "                        output = model(trainx.transpose(1, 3), adj_mx)\n",
    "                        output = output[:, hori, :, 0:1]\n",
    "                        # print(output.shape)\n",
    "\n",
    "                        \n",
    "\n",
    "                        check(sample, arr)\n",
    "                        trainx = torch.Tensor(sample).to(device)\n",
    "                        output2 = model(trainx.transpose(1, 3), adj_mx)\n",
    "                        output2 = output2[:, hori, :, 0:1]\n",
    "\n",
    "                        MAE= util.masked_mae(output2, output, 0.0)\n",
    "                        MAPE= util.masked_mape(output2, output, 0.0)\n",
    "                        RMSE= util.masked_rmse(output2, output, 0.0)\n",
    "                        \n",
    "                        amae[hori]+= MAE.item()\n",
    "                        amape[hori]+= MAPE.item()\n",
    "                        armse[hori]+= RMSE.item()\n",
    "                        \n",
    "        print(\"Done: \", iter)\n",
    "\n",
    "for hori in range(12):\n",
    "        print(f'Horizon: {hori + 1} ,MAE: {amae[hori]/temp:.4f}, MAPE: {amape[hori]/temp:.4f}, RMSE: {armse[hori]/temp:.4f} \\n')\n",
    "                # print (output)\n",
    "                # print(\"---------------------------------------------\")\n",
    "                # print(output2)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        # print(f'Mean:  ,MAE: {np.mean(amae)}, MAPE: {np.mean(amape)}, RMSE: {np.mean(armse)} ')\n",
    "                \n",
    "        # sorted_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "        # for result in sorted_results:\n",
    "        #     hori, MAE, MAPE, RMSE = result\n",
    "        #     print(f'Horizon: {hori} ,MAE: {MAE:.4f}, MAPE: {MAPE:.4f}, RMSE: {RMSE:.4f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "torch.Size([1, 12, 325, 2])\n",
      "Horizon: 1 ,MAE: 0.3894, MAPE: 0.7176, RMSE: 0.4482 \n",
      "\n",
      "Horizon: 2 ,MAE: 0.0202, MAPE: 1.4719, RMSE: 0.0254 \n",
      "\n",
      "Horizon: 3 ,MAE: 0.0218, MAPE: 1.5451, RMSE: 0.0273 \n",
      "\n",
      "Horizon: 4 ,MAE: 0.0201, MAPE: 1.4420, RMSE: 0.0251 \n",
      "\n",
      "Horizon: 5 ,MAE: 0.0197, MAPE: 0.3604, RMSE: 0.0249 \n",
      "\n",
      "Horizon: 6 ,MAE: 0.0198, MAPE: 2.0017, RMSE: 0.0247 \n",
      "\n",
      "Horizon: 7 ,MAE: 0.0208, MAPE: 0.8142, RMSE: 0.0263 \n",
      "\n",
      "Horizon: 8 ,MAE: 0.0196, MAPE: 0.6974, RMSE: 0.0247 \n",
      "\n",
      "Horizon: 9 ,MAE: 0.0219, MAPE: 0.3718, RMSE: 0.0271 \n",
      "\n",
      "Horizon: 10 ,MAE: 0.0202, MAPE: 27.3869, RMSE: 0.0256 \n",
      "\n",
      "Horizon: 11 ,MAE: 0.0192, MAPE: 0.4097, RMSE: 0.0243 \n",
      "\n",
      "Horizon: 12 ,MAE: 0.0199, MAPE: 0.0877, RMSE: 0.0248 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check(sample, arr):\n",
    "    for i, index in enumerate(arr): \n",
    "        sample[:, i, index, :]= 0.0\n",
    "\n",
    "\n",
    "def normalize_tensor(data):\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    # X_scaled = normalized_data * (max_val - min_val) + min_val\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "sample = np.load('./saved_models/PEMS_BAY/sample_PEMS_BAY_1.npz')['arr_0']\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(args.device)\n",
    "_, _, adj_mx = util.load_adj('store/adj_mx_bay.pkl')\n",
    "\n",
    "adj_mx = torch.tensor(adj_mx).to(device)\n",
    "\n",
    "\n",
    "model = BeatsODE(in_dim=2, out_dim=1, seq_len=12).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('save_blackbox/G_T_model_15.pth'))\n",
    "\n",
    "\n",
    "print(\"start training...\", flush=True)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "mask = np.load('./saved_models/PEMS_BAY/saliency_PEMS_BAY_1.npz')['arr_0']\n",
    "# for i, sample in enumerate(samples):\n",
    "sample= torch.Tensor(sample).unsqueeze(0)\n",
    "trainx = torch.Tensor(sample).to(device)\n",
    "print(trainx.shape)\n",
    "\n",
    "mask= mask[0, 0, :, :]\n",
    "# print(mask.shape)\n",
    "mask= normalize_tensor(mask)\n",
    "mask= mask.T\n",
    "mask = torch.Tensor(mask)\n",
    "# print(\"mask: \", mask)\n",
    "top_k = torch.topk(mask, 30, dim=1, sorted= True)\n",
    "index_ = top_k.indices\n",
    "arr= []\n",
    "for i in range(index_.shape[0]):\n",
    "        row= np.full((325, ), True)\n",
    "        row[index_[i]]= False\n",
    "        arr.append(row)\n",
    "arr = np.array(arr)\n",
    "\n",
    "amae= [0]*12\n",
    "amape= [0]*12\n",
    "armse= [0]*12\n",
    "temp = 10\n",
    "\n",
    "for hori in range(12):\n",
    "        with torch.no_grad():\n",
    "                output = model(trainx.transpose(1, 3), adj_mx)\n",
    "                output = output[:, hori, :, 0:1]\n",
    "                # print(output.shape)\n",
    "\n",
    "                \n",
    "\n",
    "                check(sample, arr)\n",
    "                trainx = torch.Tensor(sample).to(device)\n",
    "                output2 = model(trainx.transpose(1, 3), adj_mx)\n",
    "                output2 = output2[:, hori, :, 0:1]\n",
    "\n",
    "                MAE= util.masked_mae(output2, output, 0.0)\n",
    "                MAPE= util.masked_mape(output2, output, 0.0)\n",
    "                RMSE= util.masked_rmse(output2, output, 0.0)\n",
    "                \n",
    " \n",
    "\n",
    "\n",
    "                print(f'Horizon: {hori + 1} ,MAE: {MAE:.4f}, MAPE: {MAPE:.4f}, RMSE: {RMSE:.4f} \\n')\n",
    "                # print (output)\n",
    "                # print(\"---------------------------------------------\")\n",
    "                # print(output2)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        # print(f'Mean:  ,MAE: {np.mean(amae)}, MAPE: {np.mean(amape)}, RMSE: {np.mean(armse)} ')\n",
    "                \n",
    "        # sorted_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "        # for result in sorted_results:\n",
    "        #     hori, MAE, MAPE, RMSE = result\n",
    "        #     print(f'Horizon: {hori} ,MAE: {MAE:.4f}, MAPE: {MAPE:.4f}, RMSE: {RMSE:.4f} \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
